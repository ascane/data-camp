{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> RAMP on qualitative and quantitative non-invasive monitoring of anti-cancer drugs </h2>\n",
    "\n",
    "<i> M2 Data Science - Chia-Man Hung - chia-man.hung@polytechnique.edu </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This document aims at presenting my approaches to Data Camp 2016. http://www.ramp.studio/ <br />\n",
    "The code source can be found at on my github. https://github.com/ascane/data-camp <br />\n",
    "I will only include code that I wrote myself (not from the starting kit). Since I will not copy and load the dataset, the code below cannot be run directly from this file. Please refer to the code source in case you would like to run it yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides the plots provided by the starting kit, I tried to visualize the data myself. Below is a plot of data with the same molecule and the same concentration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for mol in np.unique(molecule):\n",
    "    for c in np.unique(concentration):\n",
    "        if len(X[(concentration == c) & (molecule == mol), :]) > 0:\n",
    "            plt.figure()\n",
    "            plt.plot(X[(concentration == c) & (molecule == mol), :].T)\n",
    "            plt.title(mol + \" - %s - %s samples.\" % (c, X[(concentration == c) & (molecule == mol), :].shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building my own model: Gaussian distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the most original part of this document. You may like to take a close look at it.\n",
    "\n",
    "### Principle\n",
    "The idea behind this model is to group data with the same molecule and the same concentration and estimate the probability distribution of each group. More specifically, in the feature_extractor_clf, we pass the group as a feature to the classifier. In the classifier, for each group, we compute a Gaussian distribution (its mean and its covariance) based on the data in a large dimension (number of frequencies chosen).\n",
    "\n",
    "Our goal is that given a test data, we want to predict the probability of having a certain molecule. We do it by computing the probability of belonging to each group based on the estimated distribution of each group. \n",
    "\n",
    "$\\forall mol \\in molecules, p(mol | test data)$ is proportional to $p(test data | mol) p(mol) = \\sum_{c} p(test data | mol , c) p(mol, c)$\n",
    "\n",
    "We estimate $p(test data | mol , c)$ by the precomputed Gaussian distribution and the $p(mol, c)$ by the proportion of the train data in each group.\n",
    "\n",
    "### Numerical issues\n",
    "In fact, the covariance matrices computed this way have no chance to be invertible because the number of frequencies chosen is much greater than the number of train data we have. However, to compute the probability, we need it to be invertible. A classical trick consists of adding an identity matrix. Another issue is then how much we add. If we add too little, some of the covariance matrices are still not invertible; if we add to much, they are almost identity. We started by tweaking its weight and observing the numerical values. Later, we found that this could be optimized by quadratic programming with linear constraints (cvxopt). Theoretical details are provided in the Reference section.\n",
    "\n",
    "The number of frequencies chosen is another parameter to tune. In the local case, there are about 1800 different frequencies. Such a big number is infeasible for the matrix computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature_extractor_clf.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class FeatureExtractorClf(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X_df, y_df):\n",
    "        self.X_df_fit = X_df\n",
    "        self.y_df_fit = y_df\n",
    "        pass\n",
    "    \n",
    "    def transform(self, X_df):\n",
    "        valueArray = X_df['spectra'].values\n",
    "        for i in range(len(valueArray)):\n",
    "            valueArray[i] = [valueArray[i][ind] for ind in range(len(valueArray[i])) if ind % 20 == 0]\n",
    "        if X_df is self.X_df_fit:\n",
    "            return zip(valueArray, self.y_df_fit.values)\n",
    "        return zip(valueArray, [None] * len(valueArray))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classifier.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "class Classifier(BaseEstimator):\n",
    "    def __init__(self):\n",
    "        self.group_m = []\n",
    "        self.group_c = []\n",
    "        self.group_mean = []\n",
    "        self.group_cov = []\n",
    "        self.group_pdf = []\n",
    "        self.group_size = []\n",
    "        self.group_n = 0\n",
    "        self.group_total_size = 0;\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        epsilon = 3.5 * 1e-7\n",
    "        n = len(X)\n",
    "        self.group_total_size = n\n",
    "        X_values = [x[0] for x in X]\n",
    "        X_molecules = [x[1][0] for x in X]\n",
    "        X_concentrations = [x[1][1] for x in X]\n",
    "        molecules = np.unique(X_molecules)\n",
    "        concentrations = np.unique(X_concentrations)\n",
    "        for m in molecules:\n",
    "            for c in concentrations:\n",
    "                mc_values = [X_values[i] for i in range(n) if X_molecules[i] == m and X_concentrations[i] == c]\n",
    "                if len(mc_values) > 0:\n",
    "                    self.group_m.append(m)\n",
    "                    self.group_c.append(c)\n",
    "                    self.group_mean.append([0] * len(mc_values[0]))\n",
    "                    for i in range(len(mc_values[0])):\n",
    "                        self.group_mean[self.group_n][i] = np.mean([x[i] for x in mc_values])\n",
    "                    self.group_cov.append(np.cov(np.transpose(mc_values)))\n",
    "                    for i in range(len(mc_values[0])):\n",
    "                        for j in range(len(mc_values[0])):\n",
    "                            self.group_cov[self.group_n][i][j] += epsilon\n",
    "                        self.group_cov[self.group_n][i][i] += epsilon\n",
    "                    self.group_pdf.append(multivariate_normal(mean=self.group_mean[self.group_n], cov=self.group_cov[self.group_n]))\n",
    "                    self.group_size.append(len(mc_values))\n",
    "                    self.group_n += 1\n",
    "                    \n",
    "                    plt.plot(self.group_mean[self.group_n - 1], label=\"%s %s\" % (m, c))\n",
    "                \n",
    "\n",
    "    def predict(self, X):\n",
    "        pass\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        mol = ['A', 'B', 'Q', 'R']\n",
    "        result = []\n",
    "        for i in range(len(X)):\n",
    "            result.append([0,0,0,0])\n",
    "            m = -1\n",
    "            for j in range(self.group_n):\n",
    "                for k in range(len(mol)):\n",
    "                    if self.group_m[j] == mol[k]:\n",
    "                        m = k\n",
    "                result[i][m] += self.group_pdf[j].pdf(X[i][0]) * self.group_size[j] / float(self.group_total_size)\n",
    "            prob_sum = sum(result[i])\n",
    "            for j in range(4):\n",
    "                result[i][j] /= prob_sum\n",
    "            \n",
    "        return np.array(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local results\n",
    "\n",
    "Below is the result I got running it locally.\n",
    "\n",
    "error = 0.12 <br />\n",
    "classification report: <br />\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          A       0.94      0.97      0.95        63\n",
    "          B       0.94      0.76      0.84        45\n",
    "          Q       0.90      0.90      0.90        40\n",
    "          R       0.76      0.87      0.81        52\n",
    "\n",
    "avg / total       0.89      0.88      0.88       200 <br />\n",
    "\n",
    "confusion matrix: <br />\n",
    " [[61  0  0  2] <br />\n",
    " [ 1 34  1  9] <br />\n",
    " [ 1  0 36  3] <br />\n",
    " [ 2  2  3 45]] <br />\n",
    " \n",
    " Note that the two parameters mentioned above (weight of the identity matrix, number of frequencies chosen) can still be tuned to obtain better results. This error is comparable to the untuned RandomForestClassifer provided in the starting kit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission on the server\n",
    "\n",
    "I tried to submit this even though it does not seem to perform much better than the RandomForestClassifier provided by the starting kit. However, due to a descrepency between the local test and the server, my submission failed. (In the user test fe_clf.fit() received the pandas table y_df, whereas on the server we sent only the molecule types in a 1D numpy array.) After the starting kit got updated, I had only one submission left. Therefore, I did not have a chance to submit it on the server.\n",
    "\n",
    "I wanted to implement the same idea in the regressor but I did not continue on this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "A well-conditioned estimator for large-dimensional covariance matrices, O. Ledoit, M. Wolf, February 2001. <br />\n",
    "http://perso.ens-lyon.fr/patrick.flandrin/LedoitWolf_JMA2004.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation (Hyperparameters tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I started by using GridSearchCV in sklearn.model_selection, but the results were only given at the end of the training. It was not very convenient and I could not estimate when it would terminate. My workaround using lots of for loops and printing all the results out as seen below. There are a lot of parameters to tune and it is impossible to run them all at a time. First, I selected some of them and set the step in range to be quite big to to have a vague idea. Then, I reduced the range and decreased the step sizes to have more precise values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "class Classifier(BaseEstimator):\n",
    "    def __init__(self, n_components, C, degree, coef0, gamma, kernel):\n",
    "        self.n_components = n_components\n",
    "        self.C = C\n",
    "        self.degree = degree\n",
    "        self.coef0 = coef0\n",
    "        self.gamma = gamma\n",
    "        self.kernel = kernel\n",
    "        self.clf = Pipeline([\n",
    "            ('pca', PCA(n_components=self.n_components)), \n",
    "            ('svc', SVC(C=self.C, kernel=self.kernel, degree=self.degree, gamma=self.gamma, coef0=self.coef0, shrinking=True, probability=True, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape=None, random_state=42))\n",
    "        ])\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.clf.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.clf.predict(X)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.clf.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def frange(x, y, jump):\n",
    "    while x < y:\n",
    "        yield x\n",
    "        x += jump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import ShuffleSplit, cross_val_score, train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "labels = np.array(['A', 'B', 'Q', 'R'])\n",
    "\n",
    "def train_test_model_clf(X_df, y_df, skf_is, FeatureExtractor, Classifier, n_components, C, degree, coef0, gamma, kernel):\n",
    "    train_is, test_is = skf_is\n",
    "    X_train_df = X_df.iloc[train_is].copy()                                  \n",
    "    y_train_df = y_df.iloc[train_is].copy()\n",
    "    y_train_clf = y_train_df['molecule'].values\n",
    "    X_test_df = X_df.iloc[test_is].copy()                                    \n",
    "    y_test_df = y_df.iloc[test_is].copy() \n",
    "    y_test_clf = y_test_df['molecule'].values \n",
    "    # Feature extraction\n",
    "    fe_clf = FeatureExtractor()\n",
    "    fe_clf.fit(X_train_df, y_train_df)\n",
    "    X_train_array_clf = fe_clf.transform(X_train_df)\n",
    "    X_test_array_clf = fe_clf.transform(X_test_df)\n",
    "    # Train\n",
    "    clf = Classifier(n_components, C, degree, coef0, gamma, kernel)\n",
    "    clf.fit(X_train_array_clf, y_train_clf)\n",
    "    # Test \n",
    "    y_proba_clf = clf.predict_proba(X_test_array_clf)                        \n",
    "    y_pred_clf = labels[np.argmax(y_proba_clf, axis=1)]                      \n",
    "    error = 1 - accuracy_score(y_test_clf, y_pred_clf)\n",
    "    print('n_components = %s - C = %s - degree = %s - coef0 = %s - gamma = %s - kernel = %s' % (n_components, C, degree, coef0, gamma, kernel))\n",
    "    print('error = %s' % error)\n",
    "    return error\n",
    "\n",
    "\n",
    "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "\n",
    "best_n_components, best_C, best_degree, best_coef, best_gamma, best_kernel, best_error = -1, -1, -1, -1, -1, '', 100\n",
    "for n_components in range(8, 12, 1):\n",
    "    for C in frange(8.3, 8.4, 0.1):\n",
    "        for degree in range(10, 11, 2):\n",
    "            for coef0 in frange(3.4, 3.5, 0.1):\n",
    "                for gamma in frange(0.05, 0.06, 0.01):\n",
    "                    for kernel in kernels:\n",
    "                        error = []\n",
    "                        for t in range(5):\n",
    "                            skf = ShuffleSplit(n_splits=2, test_size=0.2, random_state=57)  \n",
    "                            skf_is = list(skf.split(X_df))[0]\n",
    "                            error.append(train_test_model_clf(X_df, y_df, skf_is, FeatureExtractorClf, Classifier, n_components, C, degree, coef0, gamma, kernel))\n",
    "                        if np.mean(error) < best_error:\n",
    "                            best_error = np.mean(error)\n",
    "                            best_n_components, best_C, best_degree, best_coef0, best_gamma, best_kernel = n_components, C, degree, coef0, gamma, kernel\n",
    "\n",
    "print('best_n_components = %s' % best_n_components)\n",
    "print('best_C = %s' % best_C)\n",
    "print('best_degree = %s' % best_degree)\n",
    "print('best_coef0 = %s' % best_coef0)\n",
    "print('best_gamma = %s' % best_gamma)\n",
    "print('best_kernel = %s' % best_kernel)\n",
    "print('best_error = %s' % best_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local results\n",
    "\n",
    "Below are the intermediate results found running the previous code. The unspecified parameters are set to default. <br />\n",
    "\n",
    "best_n_components = 100, best_C = 8, best_degree = 10, best_coef0 = 3, best_kernel = poly, best_error = 0.038 <br />\n",
    "best_n_components = 100, best_C = 7, best_degree = 10, best_coef0 = 3, best_kernel = poly, best_error = 0.039 <br />\n",
    "best_n_components = 100, best_C = 8.3, best_degree = 10, best_coef0 = 3.5, best_kernel = poly, best_error = 0.033 <br />\n",
    "best_n_components = 100, best_C = 8.1, best_degree = 10, best_coef0 = 3.4, best_kernel = poly, best_error = 0.037 <br />\n",
    "best_n_components = 100, best_C = 8.3, best_degree = 10, best_coef0 = 3.4, best_gamma = 0.1, best_kernel = poly, best_error = 0.033 <br />\n",
    "best_n_components = 100, best_C = 8.3, best_degree = 10, best_coef0 = 3.4, best_gamma = 0.05, best_kernel = poly, best_error = 0.031 <br />\n",
    "best_n_components = 100, best_C = 8.3, best_degree = 10, best_coef0 = 3.4, best_gamma = 0.02, best_kernel = poly, best_error = 0.035 <br />\n",
    "best_n_components = 100, best_C = 8.3, best_degree = 10, best_coef0 = 3.4, best_gamma = 0.04, best_kernel = poly, best_error = 0.03 <br />\n",
    "best_n_components = 100, best_C = 8.3, best_degree = 10, best_coef0 = 3.4, best_gamma = 0.05, best_kernel = poly, best_error = 0.034 <br />\n",
    "best_n_components = 10, best_C = 8.3, best_degree = 10, best_coef0 = 3.4, best_gamma = 0.05, best_kernel = poly, best_error = 0.02 <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor                           \n",
    "from sklearn.decomposition import PCA                                            \n",
    "from sklearn.pipeline import Pipeline                                            \n",
    "from sklearn.base import BaseEstimator                                           \n",
    "import numpy as np                                                               \n",
    "                           \n",
    "class Regressor(BaseEstimator):                                                  \n",
    "    def __init__(self, n_components, n_estimators, learning_rate, loss, subsample, min_samples_split, min_samples_leaf, min_weight_fraction_leaf, max_depth, min_impurity_split, alpha):                                                       \n",
    "        self.n_components = n_components                                         \n",
    "        self.n_estimators = n_estimators                                             \n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss = loss\n",
    "        self.subsample = subsample\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.min_weight_fraction_leaf = min_weight_fraction_leaf\n",
    "        self.max_depth = max_depth\n",
    "        self.min_impurity_split = min_impurity_split\n",
    "        self.alpha = alpha\n",
    "        self.list_molecule = ['A', 'B', 'Q', 'R']                                \n",
    "        self.dict_reg = {}                                                       \n",
    "        for mol in self.list_molecule:                                           \n",
    "            self.dict_reg[mol] = Pipeline([                                      \n",
    "                ('pca', PCA(n_components=self.n_components)),                    \n",
    "                ('reg', GradientBoostingRegressor(\n",
    "                    n_estimators=self.n_estimators,                              \n",
    "                    learning_rate=self.learning_rate, \n",
    "                    loss=self.loss, # 'ls'\n",
    "                    subsample=self.subsample, #1.0\n",
    "                    criterion='friedman_mse',\n",
    "                    min_samples_split=self.min_samples_split, # 2\n",
    "                    min_samples_leaf=self.min_samples_leaf, # 1\n",
    "                    min_weight_fraction_leaf=self.min_weight_fraction_leaf, # 0.0\n",
    "                    max_depth=self.max_depth, # 3\n",
    "                    min_impurity_split=self.min_impurity_split, # 1e-07\n",
    "                    init=None,\n",
    "                    max_features='sqrt',\n",
    "                    alpha=self.alpha, # 0.9\n",
    "                    max_leaf_nodes=None,\n",
    "                    warm_start=False,\n",
    "                    random_state=42))                                            \n",
    "            ])                                                                   \n",
    "                                                                                 \n",
    "    def fit(self, X, y):                                                         \n",
    "        for i, mol in enumerate(self.list_molecule):                             \n",
    "            ind_mol = np.where(np.argmax(X[:, -4:], axis=1) == i)[0]             \n",
    "            XX_mol = X[ind_mol]                                                  \n",
    "            y_mol = y[ind_mol].astype(float)                                     \n",
    "            self.dict_reg[mol].fit(XX_mol, np.log(y_mol))                        \n",
    "                                                                                 \n",
    "    def predict(self, X):                                                        \n",
    "        y_pred = np.zeros(X.shape[0])                                            \n",
    "        for i, mol in enumerate(self.list_molecule):                             \n",
    "            ind_mol = np.where(np.argmax(X[:, -4:], axis=1) == i)[0]             \n",
    "            XX_mol = X[ind_mol].astype(float)                                    \n",
    "            y_pred[ind_mol] = np.exp(self.dict_reg[mol].predict(XX_mol))         \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mare_score(y_true, y_pred):                                                  \n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) \n",
    "\n",
    "def train_test_model(X_df, y_df, skf_is, FeatureExtractorClf, Classifier, FeatureExtractorReg, RegressorClassifier, n_components_2, n_estimators, learning_rate, loss, subsample, min_samples_split, min_samples_leaf, min_weight_fraction_leaf, max_depth, min_impurity_split, alpha, n_components=10, C=8.3, degree=10, coef0=3.4, gamma=0.05, kernel='poly'):\n",
    "    train_is, test_is = skf_is\n",
    "    X_train_df = X_df.iloc[train_is].copy()                                  \n",
    "    y_train_df = y_df.iloc[train_is].copy()                                  \n",
    "    X_test_df = X_df.iloc[test_is].copy()                                    \n",
    "    y_test_df = y_df.iloc[test_is].copy()                                    \n",
    "    y_train_clf = y_train_df['molecule'].values                              \n",
    "    y_train_reg = y_train_df['concentration'].values                         \n",
    "    y_test_clf = y_test_df['molecule'].values                                \n",
    "    y_test_reg = y_test_df['concentration'].values                           \n",
    "\n",
    "    # Classification\n",
    "    fe_clf = FeatureExtractorClf()                     \n",
    "    fe_clf.fit(X_train_df, y_train_df)                                       \n",
    "    X_train_array_clf = fe_clf.transform(X_train_df)                         \n",
    "    X_test_array_clf = fe_clf.transform(X_test_df)                           \n",
    "                                                                                 \n",
    "    clf = Classifier(n_components, C, degree, coef0, gamma, kernel)                                          \n",
    "    clf.fit(X_train_array_clf, y_train_clf)                                  \n",
    "    y_proba_clf = clf.predict_proba(X_test_array_clf)                        \n",
    "    y_pred_clf = labels[np.argmax(y_proba_clf, axis=1)]                      \n",
    "    error = 1 - accuracy_score(y_test_clf, y_pred_clf)                       \n",
    "    \n",
    "    # Regression\n",
    "    fe_reg = FeatureExtractorReg()                     \n",
    "    for i, label in enumerate(labels):\n",
    "        # For training, we use \n",
    "        X_train_df.loc[:, label] = (y_train_df['molecule'] == label)         \n",
    "        X_test_df.loc[:, label] = y_proba_clf[:, i]                          \n",
    "    fe_reg.fit(X_train_df, y_train_reg)                                      \n",
    "    X_train_array_reg = fe_reg.transform(X_train_df)                         \n",
    "    X_test_array_reg = fe_reg.transform(X_test_df)                           \n",
    "                                                                                 \n",
    "    reg = Regressor(n_components_2, n_estimators, learning_rate, loss, subsample, min_samples_split, min_samples_leaf, min_weight_fraction_leaf, max_depth, min_impurity_split, alpha)                                              \n",
    "    reg.fit(X_train_array_reg, y_train_reg)                               \n",
    "    y_pred_reg = reg.predict(X_test_array_reg)\n",
    "    mare = mare_score(y_test_reg, y_pred_reg)\n",
    "    print('n_components_2 = %s - n_estimator = %s - learning_rate = %s - loss = %s - subsample = %s - min_samples_split = %s - min_samples_leaf = %s - min_weight_fraction_leaf = %s - max_depth = %s - min_impurity_split = %s - alpha = %s' % (n_components_2, n_estimators, learning_rate, loss, subsample, min_samples_split, min_samples_leaf, min_weight_fraction_leaf, max_depth, min_impurity_split, alpha))\n",
    "    print('mare = ', mare)                \n",
    "    print('combined error = ', 2. / 3 * error + 1. / 3 * mare)\n",
    "    return mare\n",
    "\n",
    "\n",
    "losses = ['ls', 'lad', 'huber', 'quantile']\n",
    "\n",
    "best_n_components_2, best_n_estimators, best_learning_rate, best_loss, best_subsample, best_min_samples_split, best_min_samples_leaf, best_min_weight_fraction_leaf, best_max_depth, best_min_impurity_split, best_alpha, best_error = -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 100\n",
    "for n_components_2 in range(10, 15, 5):\n",
    "    for n_estimators in range(100, 1500, 200):\n",
    "        for learning_rate in frange(0.05, 0.25, 0.05):\n",
    "            for loss in losses:\n",
    "                for subsample in frange(0.9, 1, 0.2):\n",
    "                    for min_samples_split in range(2, 3, 1):\n",
    "                        for min_samples_leaf in range(1, 2, 1):\n",
    "                            for min_weight_fraction_leaf in frange(0, 0.1, 0.1):\n",
    "                                for max_depth in range(3, 5, 2):\n",
    "                                    for min_impurity_split in frange(1e-7, 2e-7, 1e-7):\n",
    "                                        for alpha in frange(0.9, 1.0, 0.1):\n",
    "                                            error = []\n",
    "                                            for t in range(5):\n",
    "                                                skf = ShuffleSplit(n_splits=2, test_size=0.2, random_state=57)  \n",
    "                                                skf_is = list(skf.split(X_df))[0]\n",
    "                                                error.append(train_test_model(X_df, y_df, skf_is, FeatureExtractorClf, Classifier, FeatureExtractorReg, Regressor, n_components_2, n_estimators, learning_rate, loss, subsample, min_samples_split, min_samples_leaf, min_weight_fraction_leaf, max_depth, min_impurity_split, alpha))\n",
    "                                            if np.mean(error) < best_error:\n",
    "                                                best_error = np.mean(error)\n",
    "                                                best_n_components_2, best_n_estimators, best_learning_rate, best_loss, best_subsample, best_min_samples_split, best_min_samples_leaf, best_min_weight_fraction_leaf, best_max_depth, best_min_impurity_split, best_alpha = n_components_2, n_estimators, learning_rate, loss, subsample, min_samples_split, min_samples_leaf, min_weight_fraction_leaf, max_depth, min_impurity_split, alpha\n",
    "print('best_n_components_2 = %s - best_n_estimator = %s - best_learning_rate = %s - best_loss = %s - best_subsample = %s - best_min_samples_split = %s - best_min_samples_leaf = %s - best_min_weight_fraction_leaf = %s - best_max_depth = %s - best_min_impurity_split = %s - best_alpha = %s' % (best_n_components_2, best_n_estimators, best_learning_rate, loss, subsample, min_samples_split, min_samples_leaf, min_weight_fraction_leaf, max_depth, min_impurity_split, alpha))\n",
    "print('best_mare = %s' % best_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local results\n",
    "\n",
    "The unspecified parameters are set to default.\n",
    "\n",
    "best_n_components_2 = 10 - best_n_estimator = 110 - best_learning_rate = 0.2 - best_mare = 0.15797804292\n",
    "\n",
    "best_n_components_2 = 10 - best_n_estimator = 110 - best_learning_rate = 0.15 - best_mare = 0.159668444652\n",
    "\n",
    "best_n_components_2 = 10 - best_n_estimator = 300 - best_learning_rate = 0.2 - best_mare = 0.158506996752\n",
    "\n",
    "best_n_components_2 = 10 - best_n_estimator = 300 - best_learning_rate = 0.2 - best_loss = huber - best_subsample = 0.9 - best_min_samples_split = 2 - best_min_samples_leaf = 1 - best_min_weight_fraction_leaf = 0 - best_max_depth = 3 - best_min_impurity_split = 1e-07 - best_alpha = 0.9 - best_mare = 0.152787475799\n",
    "\n",
    "best_n_components_2 = 10 - best_n_estimator = 300 - best_learning_rate = 0.2 - best_loss = huber - best_subsample = 0.9 - best_min_samples_split = 200 - best_min_samples_leaf = 1 - best_min_weight_fraction_leaf = 0 - best_max_depth = 7 - best_min_impurity_split = 1e-07 - best_alpha = 0.9 - best_mare = 0.157893457824\n",
    "\n",
    "best_n_components_2 = 10 - best_n_estimator = 300 - best_learning_rate = 0.2 - best_loss = huber - best_subsample = 0.9 - best_min_samples_split = 10 - best_min_samples_leaf = 4 - best_min_weight_fraction_leaf = 0 - best_max_depth = 7 - best_min_impurity_split = 1e-07 - best_alpha = 0.9 - best_mare = 0.169251068628\n",
    "\n",
    "best_n_components_2 = 10 - best_n_estimator = 130 - best_learning_rate = 0.2 - best_loss = huber - best_subsample = 0.9 - best_min_samples_split = 2 - best_min_samples_leaf = 1 - best_min_weight_fraction_leaf = 0 - best_max_depth = 3 - best_min_impurity_split = 1e-07 - best_alpha = 0.9 - best_mare = 0.153183626478\n",
    "\n",
    "best_n_components_2 = 10 - best_n_estimator = 140 - best_learning_rate = 0.2 - best_loss = ls - best_subsample = 0.9 - best_min_samples_split = 2 - best_min_samples_leaf = 1 - best_min_weight_fraction_leaf = 0 - best_max_depth = 3 - best_min_impurity_split = 1e-07 - best_alpha = 0.9 - best_mare = 0.151323405277\n",
    "\n",
    "best_n_components_2 = 10 - best_n_estimator = 100 - best_learning_rate = 0.2 - best_loss = ls - best_subsample = 0.9 - best_min_samples_split = 2 - best_min_samples_leaf = 1 - best_min_weight_fraction_leaf = 0 - best_max_depth = 3 - best_min_impurity_split = 1e-07 - best_alpha = 0.9 - best_mare = 0.152335430693\n",
    "\n",
    "best_n_components_2 = 10 - best_n_estimator = 700 - best_learning_rate = 0.15 - best_loss = ls - best_subsample = 0.9 - best_min_samples_split = 2 - best_min_samples_leaf = 1 - best_min_weight_fraction_leaf = 0 - best_max_depth = 3 - best_min_impurity_split = 1e-07 - best_alpha = 0.9 - best_mare = 0.151876105299"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "Cross Validation & Ensembling <br />\n",
    "http://datalab-lsml.appspot.com/lectures/08_CV_Ensembling.html <br />\n",
    "Parameter tuning Gradient boosting <br />\n",
    "https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After trying several models in sklearn, the SVC turns out to be the best for the classifier and the GradientBoostingRegressor turns out to be the best for the regressor. I didn't manage to add other features that improve the results, so the feature extractors stay the same as in the starting kit. Hyperparameters are tuned according to the best results found in the previous section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classifier.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "class Classifier(BaseEstimator):\n",
    "    def __init__(self):\n",
    "        self.n_components = 10\n",
    "        self.clf = Pipeline([\n",
    "            ('pca', PCA(n_components=self.n_components)), \n",
    "            ('svc', SVC(C=8.3, kernel='poly', degree=10, gamma=0.05, coef0=3.4, shrinking=True, probability=True, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape=None, random_state=None))\n",
    "        ])\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.clf.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.clf.predict(X)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.clf.predict_proba(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "regressor.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor                           \n",
    "from sklearn.decomposition import PCA                                            \n",
    "from sklearn.pipeline import Pipeline                                            \n",
    "from sklearn.base import BaseEstimator                                           \n",
    "import numpy as np                                                               \n",
    "                                                                                 \n",
    "                                                                                 \n",
    "class Regressor(BaseEstimator):                                                  \n",
    "    def __init__(self):                                                          \n",
    "        self.n_components = 10                                        \n",
    "        self.n_estimators = 140                                              \n",
    "        self.learning_rate = 0.2\n",
    "        self.loss = 'ls'\n",
    "        self.subsample = 0.9\n",
    "        self.min_samples_split = 2\n",
    "        self.min_samples_leaf = 1\n",
    "        self.min_weight_fraction_leaf = 0.0\n",
    "        self.max_depth = 3\n",
    "        self.min_impurity_split = 1e-07\n",
    "        self.alpha = 0.9\n",
    "        \n",
    "        self.list_molecule = ['A', 'B', 'Q', 'R']                                \n",
    "        self.dict_reg = {}                                                       \n",
    "        for mol in self.list_molecule:                                           \n",
    "            self.dict_reg[mol] = Pipeline([                                      \n",
    "                ('pca', PCA(n_components=self.n_components)),                    \n",
    "                ('reg', GradientBoostingRegressor(\n",
    "                    n_estimators=self.n_estimators,                              \n",
    "                    learning_rate=self.learning_rate, \n",
    "                    loss=self.loss, # 'ls'\n",
    "                    subsample=self.subsample, #1.0\n",
    "                    criterion='friedman_mse',\n",
    "                    min_samples_split=self.min_samples_split, # 2\n",
    "                    min_samples_leaf=self.min_samples_leaf, # 1\n",
    "                    min_weight_fraction_leaf=self.min_weight_fraction_leaf, # 0.0\n",
    "                    max_depth=self.max_depth, # 3\n",
    "                    min_impurity_split=self.min_impurity_split, # 1e-07\n",
    "                    init=None,\n",
    "                    max_features=None,\n",
    "                    alpha=self.alpha, # 0.9\n",
    "                    max_leaf_nodes=None,\n",
    "                    warm_start=False,\n",
    "                    random_state=42))                                            \n",
    "            ])                                                                   \n",
    "                                                                                 \n",
    "    def fit(self, X, y):                                                         \n",
    "        for i, mol in enumerate(self.list_molecule):                             \n",
    "            ind_mol = np.where(np.argmax(X[:, -4:], axis=1) == i)[0]             \n",
    "            XX_mol = X[ind_mol]                                                  \n",
    "            y_mol = y[ind_mol].astype(float)                                     \n",
    "            self.dict_reg[mol].fit(XX_mol, np.log(y_mol))                        \n",
    "                                                                                 \n",
    "    def predict(self, X):                                                        \n",
    "        y_pred = np.zeros(X.shape[0])                                            \n",
    "        for i, mol in enumerate(self.list_molecule):                             \n",
    "            ind_mol = np.where(np.argmax(X[:, -4:], axis=1) == i)[0]             \n",
    "            XX_mol = X[ind_mol].astype(float)                                    \n",
    "            y_pred[ind_mol] = np.exp(self.dict_reg[mol].predict(XX_mol))         \n",
    "        return y_pred  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local results\n",
    "\n",
    "Results from !python user_test_submission.py <br />\n",
    "\n",
    "Reading file ... <br />\n",
    "Training file ... <br />\n",
    "-------------------------- <br />\n",
    "error = 0.02 <br />\n",
    "('mare = ', 0.16059234361494051) <br />\n",
    "('combined error = ', 0.066864114538313521) <br />\n",
    "-------------------------- <br />\n",
    "error = 0.04 <br />\n",
    "('mare = ', 0.1562354813434049) <br />\n",
    "('combined error = ', 0.078745160447801651) <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission on the server\n",
    "\n",
    "Combined error = 0.074 <br />\n",
    "error = 0.032 <br />\n",
    "mare = 0.157 <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
